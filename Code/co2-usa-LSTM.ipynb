{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-21T22:08:53.269982Z","iopub.execute_input":"2022-05-21T22:08:53.270313Z","iopub.status.idle":"2022-05-21T22:08:53.297256Z","shell.execute_reply.started":"2022-05-21T22:08:53.270237Z","shell.execute_reply":"2022-05-21T22:08:53.296609Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"dataset = pd.read_csv('../input/usa-co2/USA_CO2.csv')\ndataset","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2022-05-21T22:08:56.481013Z","iopub.execute_input":"2022-05-21T22:08:56.481564Z","iopub.status.idle":"2022-05-21T22:08:56.534763Z","shell.execute_reply.started":"2022-05-21T22:08:56.481525Z","shell.execute_reply":"2022-05-21T22:08:56.533968Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"dataset.head(5)","metadata":{"execution":{"iopub.status.busy":"2022-05-21T22:08:59.167329Z","iopub.execute_input":"2022-05-21T22:08:59.168053Z","iopub.status.idle":"2022-05-21T22:08:59.190631Z","shell.execute_reply.started":"2022-05-21T22:08:59.168015Z","shell.execute_reply":"2022-05-21T22:08:59.189887Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"dataset.drop(dataset.columns.difference(['year','co2']), 1, inplace=True)\ndataset=dataset.rename(columns={'year':'yr', 'co2':'y'})\ndataset.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-21T22:09:03.181166Z","iopub.execute_input":"2022-05-21T22:09:03.181465Z","iopub.status.idle":"2022-05-21T22:09:03.196992Z","shell.execute_reply.started":"2022-05-21T22:09:03.181433Z","shell.execute_reply":"2022-05-21T22:09:03.195935Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"year_max = dataset['yr'].max()\nyear_min = dataset['yr'].min()\n\nlength = year_max - year_min +1\n\n#creating a list of years to store all year values\nyr_val = dataset['yr']\n\ndataset['ds'] = [str(yr)+'/1/1' for yr in yr_val]\n\n\ndataset['ds'] = pd.to_datetime(dataset['ds'])\ndataset.head(5)","metadata":{"execution":{"iopub.status.busy":"2022-05-21T22:09:05.315249Z","iopub.execute_input":"2022-05-21T22:09:05.315719Z","iopub.status.idle":"2022-05-21T22:09:05.331077Z","shell.execute_reply.started":"2022-05-21T22:09:05.315680Z","shell.execute_reply":"2022-05-21T22:09:05.330459Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"years= dataset['yr'].to_list()","metadata":{"execution":{"iopub.status.busy":"2022-05-21T22:25:06.924520Z","iopub.execute_input":"2022-05-21T22:25:06.924778Z","iopub.status.idle":"2022-05-21T22:25:06.929771Z","shell.execute_reply.started":"2022-05-21T22:25:06.924751Z","shell.execute_reply":"2022-05-21T22:25:06.929048Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"train_data = dataset['y'].to_list()\ndata_lstm = np.asarray(train_data)","metadata":{"execution":{"iopub.status.busy":"2022-05-21T22:18:31.737933Z","iopub.execute_input":"2022-05-21T22:18:31.738235Z","iopub.status.idle":"2022-05-21T22:18:31.743179Z","shell.execute_reply.started":"2022-05-21T22:18:31.738203Z","shell.execute_reply":"2022-05-21T22:18:31.742318Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"\n\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Create training and testing sets\nratio = 0.9\ninput_length = 5    # Here we take 3 years as input length\noutput_length = 1\n\n\n#data_lstm.reset_index(inplace=True,drop=True)\n#data_lstm = data_lstm.values\n\n# # Scale data between 0 and 1\nscaler = MinMaxScaler()\ndata_lstm = scaler.fit_transform(np.reshape(data_lstm,(-1,1)))\n#data_lstm = np.reshape(data_lstm,(-1,1))\n# Split between training and testing sets\nsplit = (int)(np.ceil(ratio*len(data_lstm)))\nx_train = [data_lstm[i:i+input_length] for i in range(split-input_length)]\ny_train = [data_lstm[i+input_length][0] for i in range(split-input_length)]\nx_test = [data_lstm[i+split:i+split+input_length] for i in range(len(data_lstm)-split-input_length)]\ny_test = [data_lstm[i+split+input_length][0] for i in range(len(data_lstm)-split-input_length)]\n\n# Check shapes and look at some of the values\nprint(np.shape(x_train),np.shape(y_train))\nprint(np.shape(x_test),np.shape(y_test))\nprint(x_train[0])\nprint(y_train[0])\nprint(x_test[0])\nprint(y_test[0])\n\n# Reshape x_train and x_test in order to be used in LSTM layers\nx_train_lstm = np.reshape(x_train, (np.shape(x_train)[0], np.shape(x_train)[1], 1))\nx_test_lstm = np.reshape(x_test, (np.shape(x_test)[0], np.shape(x_test)[1], 1))\n\nprint(np.shape(x_train_lstm))","metadata":{"execution":{"iopub.status.busy":"2022-05-21T22:18:51.933816Z","iopub.execute_input":"2022-05-21T22:18:51.934342Z","iopub.status.idle":"2022-05-21T22:18:51.951308Z","shell.execute_reply.started":"2022-05-21T22:18:51.934303Z","shell.execute_reply":"2022-05-21T22:18:51.950381Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"y_train = np.asarray(y_train)\ny_test = np.asarray(y_test)","metadata":{"execution":{"iopub.status.busy":"2022-05-21T22:22:54.098155Z","iopub.execute_input":"2022-05-21T22:22:54.098442Z","iopub.status.idle":"2022-05-21T22:22:54.103353Z","shell.execute_reply.started":"2022-05-21T22:22:54.098394Z","shell.execute_reply":"2022-05-21T22:22:54.101943Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"# Try to predict Average Temperature evolution over next decades regarding land and ocean temperatures\n\n# Import deep learning libraries\nfrom keras.models import Sequential\nfrom keras.layers.recurrent import LSTM\nfrom keras.layers.core import Dense, Activation, Dropout\nfrom tensorflow.keras.optimizers import SGD\n\n# Build our model\nlstm = Sequential()\n \n# Declare the layers\nlayers = [LSTM(units=8, input_shape=(input_length,1), activation='relu',return_sequences=True),\n          LSTM(units=2, activation='relu'),\n         Dense(output_length)]\n \n# Add the layers to the model\nfor layer in layers:\n    lstm.add(layer)\n\n# Compile our model\nlstm.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n \n# Fit the model\nhistory_lstm = lstm.fit(x_train_lstm, y_train, validation_data=(x_test_lstm,y_test), epochs=20, batch_size=16)","metadata":{"execution":{"iopub.status.busy":"2022-05-21T22:23:08.201520Z","iopub.execute_input":"2022-05-21T22:23:08.201780Z","iopub.status.idle":"2022-05-21T22:23:26.805136Z","shell.execute_reply.started":"2022-05-21T22:23:08.201752Z","shell.execute_reply":"2022-05-21T22:23:26.804449Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n# Plot loss evolution over training\nplt.plot(history_lstm.history['loss'])\nplt.plot(history_lstm.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')  \nplt.xlabel('epochs')\nplt.legend(['train','val'], loc='upper left')","metadata":{"execution":{"iopub.status.busy":"2022-05-21T22:23:54.764980Z","iopub.execute_input":"2022-05-21T22:23:54.765240Z","iopub.status.idle":"2022-05-21T22:23:54.987884Z","shell.execute_reply.started":"2022-05-21T22:23:54.765211Z","shell.execute_reply":"2022-05-21T22:23:54.987229Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"# Make some predictions over 50 years and plot them\nx_pred = [data_lstm[i:i+input_length] for i in range(len(data_lstm)-input_length)]\nx_pred = np.reshape(x_pred,(np.shape(x_pred)[0],np.shape(x_pred)[1],1))\n\n# Make prediction\npred = lstm.predict(x_pred)","metadata":{"execution":{"iopub.status.busy":"2022-05-21T22:27:06.289539Z","iopub.execute_input":"2022-05-21T22:27:06.290418Z","iopub.status.idle":"2022-05-21T22:27:06.358850Z","shell.execute_reply.started":"2022-05-21T22:27:06.290366Z","shell.execute_reply":"2022-05-21T22:27:06.358154Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"pred.shape","metadata":{"execution":{"iopub.status.busy":"2022-05-21T22:27:07.681923Z","iopub.execute_input":"2022-05-21T22:27:07.682640Z","iopub.status.idle":"2022-05-21T22:27:07.688892Z","shell.execute_reply.started":"2022-05-21T22:27:07.682604Z","shell.execute_reply":"2022-05-21T22:27:07.688037Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"# Plot results\nplt.figure(figsize=(10,10))\nplt.plot(years,scaler.inverse_transform(data_lstm),color='b')\nplt.plot([i for i in range(2015,2050)],scaler.inverse_transform(pred[-35:]),color='r')\nplt.legend(['recorded data','prediction'])\nplt.xlabel('Years')\nplt.ylabel('CO2 emmision')\nplt.title('Average land and ocean temperature forecast over next 50 years.')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-21T22:54:38.387040Z","iopub.execute_input":"2022-05-21T22:54:38.387313Z","iopub.status.idle":"2022-05-21T22:54:38.603129Z","shell.execute_reply.started":"2022-05-21T22:54:38.387284Z","shell.execute_reply":"2022-05-21T22:54:38.602453Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\n\naccuray = 1-mean_squared_error(data_lstm[205:], pred[200:])","metadata":{"execution":{"iopub.status.busy":"2022-05-21T22:32:57.635053Z","iopub.execute_input":"2022-05-21T22:32:57.635725Z","iopub.status.idle":"2022-05-21T22:32:57.640779Z","shell.execute_reply.started":"2022-05-21T22:32:57.635687Z","shell.execute_reply":"2022-05-21T22:32:57.639823Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"accuray","metadata":{"execution":{"iopub.status.busy":"2022-05-21T22:32:58.812902Z","iopub.execute_input":"2022-05-21T22:32:58.813177Z","iopub.status.idle":"2022-05-21T22:32:58.819261Z","shell.execute_reply.started":"2022-05-21T22:32:58.813145Z","shell.execute_reply":"2022-05-21T22:32:58.818437Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}