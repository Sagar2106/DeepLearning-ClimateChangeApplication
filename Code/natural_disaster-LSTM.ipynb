{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-22T00:03:19.978352Z","iopub.execute_input":"2022-05-22T00:03:19.979263Z","iopub.status.idle":"2022-05-22T00:03:19.991105Z","shell.execute_reply.started":"2022-05-22T00:03:19.979209Z","shell.execute_reply":"2022-05-22T00:03:19.990014Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"dataset = pd.read_csv('../input/natural-disaster/AllNaturalDisasters.csv')\ndataset","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2022-05-22T00:03:20.227265Z","iopub.execute_input":"2022-05-22T00:03:20.227880Z","iopub.status.idle":"2022-05-22T00:03:20.261223Z","shell.execute_reply.started":"2022-05-22T00:03:20.227844Z","shell.execute_reply":"2022-05-22T00:03:20.260203Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"#dropping columns that are not essential \ndataset.drop(columns  = ['Entity','Code'], inplace = True)","metadata":{"execution":{"iopub.status.busy":"2022-05-22T00:03:20.441573Z","iopub.execute_input":"2022-05-22T00:03:20.441878Z","iopub.status.idle":"2022-05-22T00:03:20.454563Z","shell.execute_reply.started":"2022-05-22T00:03:20.441847Z","shell.execute_reply":"2022-05-22T00:03:20.453812Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"dataset","metadata":{"execution":{"iopub.status.busy":"2022-05-22T00:06:36.047314Z","iopub.execute_input":"2022-05-22T00:06:36.047958Z","iopub.status.idle":"2022-05-22T00:06:36.060424Z","shell.execute_reply.started":"2022-05-22T00:06:36.047910Z","shell.execute_reply":"2022-05-22T00:06:36.059779Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"years= dataset['Year'].to_list()","metadata":{"execution":{"iopub.status.busy":"2022-05-22T00:03:20.781998Z","iopub.execute_input":"2022-05-22T00:03:20.782298Z","iopub.status.idle":"2022-05-22T00:03:20.788046Z","shell.execute_reply.started":"2022-05-22T00:03:20.782265Z","shell.execute_reply":"2022-05-22T00:03:20.787349Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"#extracting year and month from the date column in the dataframe\n\ntrain_dataset = dataset['Number of reported natural disasters (reported disasters)']","metadata":{"execution":{"iopub.status.busy":"2022-05-22T00:03:21.002664Z","iopub.execute_input":"2022-05-22T00:03:21.003184Z","iopub.status.idle":"2022-05-22T00:03:21.007850Z","shell.execute_reply.started":"2022-05-22T00:03:21.003113Z","shell.execute_reply":"2022-05-22T00:03:21.006454Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"train_dataset = train_dataset\ndata_lstm = np.asarray(train_dataset)\ndata_lstm.shape","metadata":{"execution":{"iopub.status.busy":"2022-05-22T00:03:21.179588Z","iopub.execute_input":"2022-05-22T00:03:21.180089Z","iopub.status.idle":"2022-05-22T00:03:21.185930Z","shell.execute_reply.started":"2022-05-22T00:03:21.180036Z","shell.execute_reply":"2022-05-22T00:03:21.185029Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"\n\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Create training and testing sets\nratio = 0.9\ninput_length = 5    # Here we take 3 years as input length\noutput_length = 1\n\n\n#data_lstm.reset_index(inplace=True,drop=True)\n#data_lstm = data_lstm.values\n\n# # Scale data between 0 and 1\nscaler = MinMaxScaler()\ndata_lstm = scaler.fit_transform(np.reshape(data_lstm,(-1,1)))\n#data_lstm = np.reshape(data_lstm,(-1,1))\n# Split between training and testing sets\nsplit = (int)(np.ceil(ratio*len(data_lstm)))\nx_train = [data_lstm[i:i+input_length] for i in range(split-input_length)]\ny_train = [data_lstm[i+input_length][0] for i in range(split-input_length)]\nx_test = [data_lstm[i+split:i+split+input_length] for i in range(len(data_lstm)-split-input_length)]\ny_test = [data_lstm[i+split+input_length][0] for i in range(len(data_lstm)-split-input_length)]\n\n# Check shapes and look at some of the values\nprint(np.shape(x_train),np.shape(y_train))\nprint(np.shape(x_test),np.shape(y_test))\nprint(x_train[0])\nprint(y_train[0])\nprint(x_test[0])\nprint(y_test[0])\n\n# Reshape x_train and x_test in order to be used in LSTM layers\nx_train_lstm = np.reshape(x_train, (np.shape(x_train)[0], np.shape(x_train)[1], 1))\nx_test_lstm = np.reshape(x_test, (np.shape(x_test)[0], np.shape(x_test)[1], 1))\n\nprint(np.shape(x_train_lstm))","metadata":{"execution":{"iopub.status.busy":"2022-05-22T00:03:21.348301Z","iopub.execute_input":"2022-05-22T00:03:21.348624Z","iopub.status.idle":"2022-05-22T00:03:21.367404Z","shell.execute_reply.started":"2022-05-22T00:03:21.348583Z","shell.execute_reply":"2022-05-22T00:03:21.366274Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"y_train = np.asarray(y_train)\ny_test = np.asarray(y_test)","metadata":{"execution":{"iopub.status.busy":"2022-05-22T00:03:21.507924Z","iopub.execute_input":"2022-05-22T00:03:21.508440Z","iopub.status.idle":"2022-05-22T00:03:21.512929Z","shell.execute_reply.started":"2022-05-22T00:03:21.508397Z","shell.execute_reply":"2022-05-22T00:03:21.512022Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Try to predict Average Temperature evolution over next decades regarding land and ocean temperatures\n\n# Import deep learning libraries\nfrom keras.models import Sequential\nfrom keras.layers.recurrent import LSTM\nfrom keras.layers.core import Dense, Activation, Dropout\nfrom tensorflow.keras.optimizers import SGD\n\n# Build our model\nlstm = Sequential()\n \n# Declare the layers\nlayers = [LSTM(units=8, input_shape=(input_length,1), activation='relu',return_sequences=True),\n          LSTM(units=2, activation='relu'),\n         Dense(output_length)]\n \n# Add the layers to the model\nfor layer in layers:\n    lstm.add(layer)\n\n# Compile our model\nlstm.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n \n# Fit the model\nhistory_lstm = lstm.fit(x_train_lstm, y_train, validation_data=(x_test_lstm,y_test), epochs=20, batch_size=16)","metadata":{"execution":{"iopub.status.busy":"2022-05-22T00:03:21.700767Z","iopub.execute_input":"2022-05-22T00:03:21.701356Z","iopub.status.idle":"2022-05-22T00:03:38.864025Z","shell.execute_reply.started":"2022-05-22T00:03:21.701314Z","shell.execute_reply":"2022-05-22T00:03:38.863147Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n# Plot loss evolution over training\nplt.plot(history_lstm.history['loss'])\nplt.plot(history_lstm.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')  \nplt.xlabel('epochs')\nplt.legend(['train','val'], loc='upper left')","metadata":{"execution":{"iopub.status.busy":"2022-05-22T00:03:38.865958Z","iopub.execute_input":"2022-05-22T00:03:38.866208Z","iopub.status.idle":"2022-05-22T00:03:39.132070Z","shell.execute_reply.started":"2022-05-22T00:03:38.866177Z","shell.execute_reply":"2022-05-22T00:03:39.131038Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Make some predictions over 50 years and plot them\nx_pred = [data_lstm[i:i+input_length] for i in range(len(data_lstm)-input_length)]\nx_pred = np.reshape(x_pred,(np.shape(x_pred)[0],np.shape(x_pred)[1],1))\n\n# Make prediction\npred = lstm.predict(x_pred)","metadata":{"execution":{"iopub.status.busy":"2022-05-22T00:04:01.215134Z","iopub.execute_input":"2022-05-22T00:04:01.215486Z","iopub.status.idle":"2022-05-22T00:04:01.292363Z","shell.execute_reply.started":"2022-05-22T00:04:01.215437Z","shell.execute_reply":"2022-05-22T00:04:01.291399Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"pred.shape","metadata":{"execution":{"iopub.status.busy":"2022-05-22T00:04:02.039525Z","iopub.execute_input":"2022-05-22T00:04:02.039852Z","iopub.status.idle":"2022-05-22T00:04:02.046270Z","shell.execute_reply.started":"2022-05-22T00:04:02.039815Z","shell.execute_reply":"2022-05-22T00:04:02.045364Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# Plot results\nplt.figure(figsize=(10,10))\nplt.plot(years,data_lstm,color='b')\nplt.plot([i for i in range(2015,2050)],pred[-35:],color='r')\nplt.legend(['recorded data','prediction'])\nplt.xlabel('Years')\nplt.ylabel('Average temperature (celsius degree)')\nplt.title('Average land and ocean temperature forecast over next 50 years.')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-22T00:06:49.240824Z","iopub.execute_input":"2022-05-22T00:06:49.241631Z","iopub.status.idle":"2022-05-22T00:06:49.485548Z","shell.execute_reply.started":"2022-05-22T00:06:49.241585Z","shell.execute_reply":"2022-05-22T00:06:49.484501Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"from sklearn import metrics\naccuray = metrics.r2_score(data_lstm[55:], pred[50:])","metadata":{"execution":{"iopub.status.busy":"2022-05-22T00:16:30.413809Z","iopub.execute_input":"2022-05-22T00:16:30.414865Z","iopub.status.idle":"2022-05-22T00:16:30.421060Z","shell.execute_reply.started":"2022-05-22T00:16:30.414803Z","shell.execute_reply":"2022-05-22T00:16:30.420027Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"accuray","metadata":{"execution":{"iopub.status.busy":"2022-05-22T00:16:32.352709Z","iopub.execute_input":"2022-05-22T00:16:32.353259Z","iopub.status.idle":"2022-05-22T00:16:32.358977Z","shell.execute_reply.started":"2022-05-22T00:16:32.353224Z","shell.execute_reply":"2022-05-22T00:16:32.358214Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}